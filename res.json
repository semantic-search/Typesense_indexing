{
    "facet_counts": [
        {
            "counts": [
                {
                    "count": 1,
                    "highlighted": "epub",
                    "value": "epub"
                },
                {
                    "count": 1,
                    "highlighted": "docx",
                    "value": "docx"
                },
                {
                    "count": 1,
                    "highlighted": "doc",
                    "value": "doc"
                }
            ],
            "field_name": "mime_type",
            "stats": {}
        }
    ],
    "found": 3,
    "hits": [
        {
            "document": {
                "date": 20201017,
                "doc_id": "5f8b4457627884d3fdc0ed7f",
                "file_name": "1.doc",
                "id": "3",
                "image_location": [],
                "labels": [],
                "mime_type": "doc",
                "scores": [],
                "text": "CCTV RECAP Main github link : https://github.com/eonr/cctv-recap/blob/master/cctv_recap.py Project description : https://devfolio.co/submissions/cctv-recap opencv-python : https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_intro/py_intro.html (from here get a basic info about opencv as well as opencv-python) Info about numpy just the intro : https://www.geeksforgeeks.org/python-numpy/ To remove background image for overlapping : https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html To capture video from camera: https://www.geeksforgeeks.org/python-opencv-capture-video-from-camera/ Track moving object in opencv : https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/ (like cars and all other human movements) Crop the moving image so to overlap on all images : https://h\u00e5ken.no/blog/cropping-3-python/"
            },
            "highlights": [
                {
                    "field": "text",
                    "snippet": "Info about numpy just <mark>the</mark> intro : https://www.geeksforgeeks.org/python-numpy/ To"
                }
            ],
            "text_match": 33488996
        },
        {
            "document": {
                "date": 20201017,
                "doc_id": "5f8b4456e2a5c0f854c0edfb",
                "file_name": "1.epub",
                "id": "2",
                "image_location": [],
                "labels": [],
                "mime_type": "epub",
                "scores": [],
                "text": "creative commons a shared culture a shared culture table of contents guide creative commons a shared culture To celebrate our 2008 fundraising campaign, Creative Commons has released \u201cA Shared Culture,\u201d a short video by renowned filmmaker Jesse Dylan. Known for helming a variety of films, music videos, and the Emmy Award-winning \u201cYes We Can\u201d Barack Obama campaign video collaboration with rapper will.i.am, Dylan created \u201cA Shared Culture\u201d to help spread the word about the Creative Commons mission. Your Reading System does not support (this) video. <video id=\"video1\" controls=\"controls\"> <source src=\"../video/shared-culture.mp4\" type=\"video/mp4\"/> <source src=\"../video/shared-culture.webm\" type=\"video/webm\"/> </video> Your Reading System does not support (this) video. In the video, some of the leading thinkers behind Creative Commons describe how the organization is helping \u201csave the world from failed sharing\u201d through free tools that enable creators to easily make their work available to the public for legal sharing and remix. Dylan puts the Creative Commons system into action by punctuating the interview footage with dozens of photos that have been offered to the public for use under CC licenses. Similarly, he used two CC-licensed instrumental pieces by Nine Inch Nails as the video\u2019s soundtrack music. These tracks, \u201c17 Ghosts II\u201d and \u201c21 Ghosts III,\u201d come from the Nine Inch Nails album Ghosts I-IV, which was released earlier this year under a Creative Commons BY-NC-SA license. (See attribution.) what does it mean to be human What does it mean to be human if we don't have a shared culture? What does a shared culture mean if we can't share it? It's only in the last 100, or 150 years or so, that we started tightly restricting how that culture gets used. Your Reading System does not support (this) video. <video id=\"video1\"> <source src=\"../video/shared-culture.mp4\" type=\"video/mp4\"/> <source src=\"../video/shared-culture.webm\" type=\"video/webm\"/> </video> Your Reading System does not support (this) video. The Internet enabled an infrastructure where anybody could participate without asking permission. We have all these new technologies that allow people to express themselves, take control of their own creative impulses, but the law's getting in the way. to save the world from failed sharing Creative Commons is designed to save the world from failed sharing. People who actually want to share stuff, who put it up on the web because they want to share it under certain terms. Your Reading System does not support (this) video. <video id=\"video1\"autoplay=\"\" role=\"button\" aria-controls=\"video1\" controls=\"\"> <source src=\"../video/shared-culture.mp4\" type=\"video/mp4\"></source> <source src=\"../video/shared-culture.webm\" type=\"video/webm\"></source> <track src=\"../captions/cc-en.vtt\" srclang=\"en\" kind=\"subtitles\" label=\"English\"></track> <track src=\"../captions/cc-fr.vtt\" srclang=\"fr\" kind=\"subtitles\" label=\"Fran\u00e7ais\"></track> </video> Your Reading System does not support (this) video. So we wanted to create a simple way, for creators to say to the world here's the freedom that I want to run with my creative work. Here are the things you're allowed to do. Can I reproduce it, can I copy it, can I put it in my text book? Can I use that photograph? Can I make a new version of it? Creative Commons gives tools to creators to make a choice about copyright. exercise your copyright in more ways Creative Commons licence can cover anything that copyright covers. Every licence says \"You need to give me attribution. I created this, give me credit for the work I did.\" The basic choices are commercial use, or not. Can you make derivative works, versions, adaptations or not? And do you want me to have to share alike? Your Reading System does not support (this) video. <video id=\"video1\" autoplay=\"\" role=\"button\" aria-controls=\"video1\" controls=\"\"> <source src=\"../video/shared-culture.mp4\" type=\"video/mp4\"></source> <source src=\"../video/shared-culture.webm\" type=\"video/webm\"></source> <track src=\"../captions/cc-en.xml\" srclang=\"en\" kind=\"subtitles\" label=\"English\"></track> <track src=\"../captions/cc-fr.xml\" srclang=\"fr\" kind=\"subtitles\" label=\"Fran\u00e7ais\"></track> </video> Your Reading System does not support (this) video. So if I take your stuff, do I have to offer it to the next person under the same terms? There's no requirement for you to do anything with your work other than what you want to do. You own the copyright to it. What we've done is given you the right to exercise your copyright in more ways, more simply. a new kind of folk culture So the idea here is to enable the creative impulses that the technology turns loose, and get the law out of the way. The work of Creative Commons is really about laying the infrastructure and ground work for this new type of culture. Your Reading System does not support (this) video. <video id=\"video1\" role=\"button\" aria-controls=\"video1\"> <source src=\"../video/shared-culture.mp4\" type=\"video/mp4\"></source> <source src=\"../video/shared-culture.webm\" type=\"video/webm\"></source> </video> Your Reading System does not support (this) video. play pause mute unmute MUTED A new kind of Folk Culture. Somebody from Delhi, somebody from New York, somebody from Singapore, can feel comfortable using photo that was created and given away by somebody in The United States, or in China, or wherever that the licenses have been extended to. With their identity being preserved. Which means that people can actually create new kinds of things, come together and build things. the entire transcript Your Reading System does not support (this) audio Your Reading System does not support (this) audio What does it mean to be human if we don't have a shared culture? What does a shared culture mean if we can't share it? It's only in the last 100, or 150 years or so, that we started tightly restricting how that culture gets used. The Internet enabled an infrastructure where anybody could participate without asking permission. We have all these new technologies that allow people to express themselves, take control of their own creative impulses, but the law's getting in the way. Creative Commons is designed to save the world from failed sharing. People who actually want to share stuff, who put it up on the web because they want to share it under certain terms. So we wanted to create a simple way, for creators to say to the world here's the freedom that I want to run with my creative work. Here are the things you're allowed to do. Can I reproduce it, can I copy it, can I put it in my text book? Can I use that photograph? Can I make a new version of it? Creative Commons gives tools to creators to make a choice about copyright. Creative Commons licence can cover anything that copyright covers. Every licence says \"You need to give me attribution. I created this, give me credit for the work I did.\" The basic choices are commercial use, or not. Can you make derivative works, versions, adaptations or not? And do you want me to have to share alike? So if I take your stuff, do I have to offer it to the next person under the same terms? There's no requirement for you to do anything with your work other than what you want to do. You own the copyright to it. What we've done is given you the right to exercise your copyright in more ways, more simply. So the idea here is to enable the creative impulses that the technology turns loose, and get the law out of the way. The work of Creative Commons is really about laying the infrastructure and ground work for this new type of culture. A new kind of Folk Culture. Somebody from Delhi, somebody from New York, somebody from Singapore, can feel comfortable using photo that was created and given away by somebody in The United States, or in China, or wherever that the licenses have been extended to. With their identity being preserved. Which means that people can actually create new kinds of things, come together and build things. Mash-ups that people can do with peoples Flicker photos. And CCmixter has allowed artists to make music together. It's really about creativity and connection. Access and control. From amateurs who simply for the love of what they are doing and they want to share it and they want other people to be able to make use of it, to commercial organizations. In the end, this will have a very successful place in the for profit economy. Creative Commons is the bridge to this future. You've got to move away from thinking about content to thinking about communities. Communities that develop around content and the sharing that the licences allow enable these communities to come together. A physical Commons like a park where anybody can enter equally, a Commons with intellectual works is actually much freer. It really is going to be the pillar for communications between people, cultural exchanges. The space for more speech, more free expression. And that's the kind of Commons we're trying to create. Creative Commons All images and music used to create this work were licenced under Creative Commons licenses Attributions: Please visit http://mirrors.creativecommons.org/asharedculture for expanded credits, including links to all the creators and CC licensed works that made this video possible "
            },
            "highlights": [
                {
                    "field": "text",
                    "snippet": "on the web because <mark>they</mark> want to share it"
                }
            ],
            "text_match": 33488996
        },
        {
            "document": {
                "date": 20201017,
                "doc_id": "5f8b44557b62ddc5b5c0ed83",
                "file_name": "1.docx",
                "id": "1",
                "image_location": [],
                "labels": [],
                "mime_type": "docx",
                "scores": [],
                "text": "https://nextcloud.com https://pimylifeup.com/raspberry-pi-nextcloud-server/ RaspDrive \u2013 Khushal -> NextCloud, PiHole Project Title: - Respdrive - Personal Cloud Drive device for your home, to store, watch and share all personal files and digital assets. Project Abstract: - Raspdrive is a is a plug and play device which when connected to home network transforms it into your own cloud drive which enables the user to share and access data easily with all the device in local network, form an intuitive and user friendly UI. The users can setup user authentication and be able to save their data to local cloud, so that it can be easily accessed and streamed in all the local devices like smart TV, laptops, phones, etc. Raspdrive provides enhanced network wide security layer with full network wide AD Blocker that blocks all the advertisements with their trackers for all the device that is connected on the local network. Raspdrive support torrent download, with the ability to manage all the download request. This keeps the data downloaded from torrent accessible over the network and secure them from vulnerability issues. TOOLS AND TECHNOLOGIES TO BE USED: - Hardware : Raspberry PI Platform: Linux Frontend: JavaScript, Vue JS, SCSS Backend: Python, Php, Config/Data: XML, JSON Scripting: Shell Database: MySQL, Server: Apache Server, Transmission Torrent Downloaded Pi \u2013 Hole NETRA \u2013 Mihir Image recog : Keras, Tensorflow Scene recog -> Pytorch Object Detection -> yolo, open cv OCR -> Keras, tensorflow, Open Cv Project Title: Netra: It is an optical electronic accessibility helper device for the blind people. Project Abstract: Netra (eyes) is a wearable device consisting of a raspberry pi and webcam designed for blind person and visually impaired persons. One can simply wear the device and then by pressing a single button the camera captures an image and performs various Deep Learning Image recognition tasks such image recognition, scene recognition and object detection. The best part here is that all the image algorithms will be developed and trained by us and we will not be using any 3rd party image recog( or 3rd party APIs). Services or pre trained models. Once all the image recog is performed the user will be able to hear the output using Text to Speech. Here also we will have our own proprietary algorithm and deep learning models for speech to text and we won\u2019t be using any 3rd party speech to text model eg. Google text to speech. The result of text to speech is also state of the art resulting in a natural voice (not a robotic voice) Also, netra comes with another unique OCR feature. With a press of second button an Image will be captured of any written document such as receipts and an ocr will be done of the image. The recognized text will be then read aloud to the user using text to speech. The best part here is the ocr accuracy. The ocr will be very accurate, so much accurate than the traditional widely used open source library \u201ctesseract\u201d. Our ocr algorithm will have an accuracy of 95%. Again, this will be our own proprietary algorithm and deep learning model. All the processing of the image recognition and ocr will be performed on the Microsoft Azure Cloud VM. Hence making Netra a IOT and smart AI edge device. ATRIS Project Title: Cross platform Desktop App to manage your personal knowledge base with audio, annotation and textual data. Project Abstract: ATRIS is personal knowledge-base management Desktop App which lets you manage your notes, lectures, meeting and any sort of audio or textual data at ease, while providing functionalities to make custom voice and drawing notes within. ATRIS provides templates for different use cases like notes and meeting for General use cases, Patient medical history transcribe for special use cases of medical practitioner. The users have option to collaborate with each other in a virtual reality space and use ATRIS within the virtual environment. They would be able to interact with each other from their avatars controlled by their body gestures. Using advanced machine learning and deep learning algorithms in both sound, natural language processing and image recognition the notes and transcripts are processed, organized and displayed in a very rich, informative and precise way in the desktop. The main functionalities of ATRIS includes: Authentication to safeguard your data. advanced semantic search to find all types of data added to ATRIS (image search, sound search, vocal search, semantic text search) Offline secure privacy focused accurate speech to text engine custom workspaces to organize Notes tags for individual notes text, keyword extraction from images, sound classification from Audio to make more accessible. Named Entity Recognition (NER) on textual data. Custom Drawing Annotation Voice Notes with smart transcribe and embedding feature. Audio Player with waveform visualizer. OCR on images to extract data from them. Export as PDF, markdown. Virtual reality Collaboration. Tools and technologies to be used: UX/UI Design Figma Figma Wireframe Draw.io Mock for mock implementation. Machine learning Model Training Platform - Azure Data Science VM Models - Posenet Frameworks \u2013 Pytorch, Tensorflow, Caffe Transformers \u2013 BERT, GPT 2, DistilBERT Desktop App Node JS Electron Python Fastapi React Web Socket Web RTC Web Audio API SQLite UNITY 3D Pitch-deck: - https://youtu.be/S1wJWpLhbTw CCTV-SUMMARIZER PROJECT TITLE: CCTV-SUMMARIZER is an IOT and Artificial Intelligence project to generate abridge synopsis from long CCTV footage. PROJECT ABSTRCT: Security CCTV cameras have become more accessible to general public, but they don\u2019t have resource to assign a dedicated person, like those for banks and big offices to go through all that long footage, also storage of such long footages from CCTV becomes too difficult and expensive for normal people like home owners and shop keepers. So here CCTV-SUMMARIZER comes to the rescue, it Summarizes long hours of CCTV video footage from static CCTV into short video clip with all events occurring simultaneously with timestamp. Due to this the hours long footage can be watched in minutes and the storage space requirement is shortened significantly. The CCTV- SUMMARIZER works along with the web app so that the user can easily view the CCTV footage from a web browser. TOOLS AND TECH: Hardware: Web Cam, Raspberry PI Computer Vision: Open CV WEB APP: JavaScript, React, HTML/ CSS (FRONTEND) Python, Flask, Gunicorn (BACKEND) SQL ALCHEMY (ORM) SQLite (DATABASE) JSON Web Tokens (Authentication) UI/UX: Photoshop, ADOBE XD, ADOE XD Wireframing Kachra Seth(just for unique name and it can be changed) Project Title: An optimal AI solution for the local garbage collection problem automated. TOOLS AND TECH: Hardware: Web Cam, Raspberry PI Computer Vision: Open CV, Mask rcnn, Pytorch SMS Service: Twilio WEB APP: JavaScript, React, HTML/ CSS (FRONTEND) Python, Flask, FastAPI(BACKEND) MYSQL (DATABASE) Project Abstract: Our project is an AI automated project as per the view of smart city where there will be cameras connected to cloud as a backend where the main automation script will be running. Here our project will be IOT enabled edge device connected to a camera that can detect garbage thrown on roads in the camera premise using advanced machine learning and deep learning models that can recognize garbage, trained on garbage dataset. When detecting any garbage it will send the image of the vicinity with its geolocation from the device to the nearest municipal corporation whose distance is shortest to the recognized garbage pile location or the admin of the premises with twilio api as an SMS and will record the request in the database. There will be a frontend webapp displaying all the garbage requests of different areas in the admin side. We will make the request in the database flagged until it detects no garbage there. The main flagship point of the project is that it can be integrated on any camera whether its governments preinstalled traffic cams as well as local residential cams so its usage not restricted to Government level but rather than expanded to the consumer level too. Reward Cycle Project Title: Recycle and get Rewarded. Problem Statement: The issue is there people litter everywhere. Cleanliness is not strictly followed say it is a roadside or public place etc. On the downside even if people do put garbage in dustbin there are only a few who will put them in correct dust bin i.e. recyclable waste in recycled dust bin and non-recyclable in non-recyclable ones. People don\u2019t feel the need to do these duties as no one is going to reward them for there extra efforts. Project Abstract: A smart garbage bin heads-up system specially focused on kids and children where there will be a raspberry pi connected with the dustbin setup which includes ultrasonic sensor, camera, motion sensor etc. The chronology of the system is that a person will come near a threshold distance of the system and this will trigger our face recognition and our system then discover the person in front of it. The person will then point the garbage in front of the camera and through this our garbage classifier is triggered and will give the feedback to the user that whether the item is Organic or Recyclable and deep learning. These all haptic feedbacks will be given through led lights which will be showing the phase of the process. And after that through motion sensor in dustbin we will check that whether the person has thrown the garbage at right bin (in organic section or in-recycle section) or not and according to that feedbacks will be given and if thrown correctly then a reward point will be given to the person which he/she can redeem as a token of appreciation. So, the main motive our project is to give motivation to children to throw the garbage correctly into the correct bins and create an awareness towards them by giving rewards for their hard work and dedication and help them to strive towards home cleanliness. Hardware: Raspberry PI, Raspberry PI Camera, Led Lights Sensors: Pyroelectric (\"Passive\") InfraRed Sensors, Ultrasonic Sensor WEB APP: Backend: Language: Python3 Flask SQLite Frontend: Deep Learning: Framework: TensorFlow"
            },
            "highlights": [
                {
                    "field": "text",
                    "snippet": "within the virtual environment. <mark>They</mark> would be able to"
                }
            ],
            "text_match": 33488996
        }
    ],
    "page": 1,
    "request_params": {
        "per_page": 10,
        "q": "the"
    },
    "search_time_ms": 0
}